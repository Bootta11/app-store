minimum_swiftwave_version: "v2.2.7"
docs:
  logo_url: "https://raw.githubusercontent.com/swiftwave-org/app-store/main/assets/logos/supabase.png"
  name: "Supabase"
  description: "Supabase is an open source Firebase alternative"
  readme_description: >
    ## What is Supabase?

    Supabase is an open source Firebase alternative. \
    Start your project with a Postgres database, Authentication, instant APIs, Edge Functions, Realtime subscriptions, Storage, and Vector embeddings.

    ## Links 

    -  Official Wesbite - [supabase.com](https://supabase.com/)

    - Documentation - [supabase.com/docs](https://supabase.com/docs/)

    - GitHub - [github.com/swiftwave-org/swiftwave](https://github.com/swiftwave-org/swiftwave)

    ## Note

    - Your Swiftwave Version should be at least v2.2.7

  iframe_video_embed: ""
  variables:
    SUPABASE_DEPLOYMENT_SERVER:
      title: "Supabase Server"
      description: "The server to deploy Supabase"
      default: ""
      type: server
      options:
    POSTGRES_DATA_STORAGE:
      title: "Postgres Data Storage"
      description: "The data storage for Postgres"
      default: ""
      type: volume
      options:
    POSTGRES_CONFIG:
      title: "Postgres Config"
      description: "The storage for configuration"
      default: ""
      type: volume
    IMGPROXY_STORAGE:
      title: "Imgproxy Storage"
      description: "The storage for Imgproxy"
      default: ""
      type: volume
    SUPABASE_BUCKET_STORAGE:
      title: "Supabase Bucket Storage"
      description: "The storage for Supabase Bucket"
      default: ""
      type: volume
    FUNCTIONS_STORAGE:
      title: "Functions Storage"
      description: "The storage for Functions"
      default: ""
      type: volume
    SUPABASE_PUBLIC_URL:
      title: "Public URL for Supabase"
      description: "i.e. http://example.com:8000"
      default: ""
      type: text
    STUDIO_DEFAULT_ORGANIZATION_NAME:
      title: "Organization Name for Studio"
      description: ""
      default: "Default Organization"
      type: text
    STUDIO_DEFAULT_PROJECT_NAME:
      title: "Project Name for Studio"
      description: ""
      default: "Default Project"
      type: text
    DASHBOARD_USERNAME:
      title: "Dashboard Username"
      description: "Don't use any special character"
      default: ""
      type: text
    DASHBOARD_PASSWORD:
      title: "Dashboard Password"
      description: "Don't use any special character"
      default: ""
      type: text
    IGNORE_JWT_GENERATE_GUIDE:
      title: "Where to get JWT Secret & Keys ?"
      description: |
        Visit this [official link](https://supabase.com/docs/guides/self-hosting/docker#generate-api-keys) and **generate JWT Secret & Keys** for your project.
      default: ""
      type: markdown
      options:
    JWT_SECRET:
      title: "JWT Secret"
      description: "The secret for JWT"
      default: ""
      type: text
    ANON_JWT_KEY:
      title: "Anon JWT Key"
      description: "The secret for Anon JWT"
      default: ""
      type: text
    SERVICE_ROLE_JWT_KEY:
      title: "Service Role JWT Key"
      description: "The secret for Service Role JWT"
      default: ""
      type: text
    FUNCTIONS_VERIFY_JWT:
      title: "Functions Verify JWT"
      description: "i.e. false"
      default: "false"
      type: options
      options:
        - title: "Yes! Verify JWT"
          value: "true"
        - title: "No! Don't Verify JWT"
          value: "false"
    GOTRUE_SITE_URL:
      title: "Site URL for GoTrue"
      description: "i.e. http://example.com:3000"
      default: ""
      type: text
    GOTRUE_API_EXTERNAL_URL:
      title: "API External URL for GoTrue"
      description: "i.e. http://example.com:8000"
      default: ""
      type: text
    GOTRUE_DISABLE_SIGNUP:
      title: "Disable Signup for GoTrue"
      description: "Disable Signup for GoTrue"
      default: "false"
      type: options
      options:
        - title: "Yes! Disable Signup"
          value: "true"
        - title: "No! Keep it enabled"
          value: "false"
    GOTRUE_MAILER_URLPATHS_CONFIRMATION:
      title: "Mailer Confirmation URL Path"
      description: "i.e. /auth/v1/verify"
      default: "/auth/v1/verify"
      type: text
    GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE:
      title: "Mailer Email Change URL Path"
      description: "i.e. /auth/v1/verify"
      default: "/auth/v1/verify"
      type: text
    GOTRUE_MAILER_URLPATHS_INVITE:
      title: "Mailer Invite URL Path"
      description: "i.e. /auth/v1/verify"
      default: "/auth/v1/verify"
      type: text
    GOTRUE_MAILER_URLPATHS_RECOVERY:
      title: "Mailer Recovery URL Path"
      description: "i.e. /auth/v1/verify"
      default: "/auth/v1/verify"
      type: text
    ENABLE_EMAIL_SIGNUP:
      title: "Enable Email Signup"
      description: "i.e. true"
      default: "true"
      type: options
      options:
        - title: "Yes! Enable Email Signup"
          value: "true"
        - title: "No! Disable Email Signup"
          value: "false"
    ENABLE_EMAIL_AUTOCONFIRM:
      title: "Enable Email Autoconfirm"
      description: "i.e. true"
      default: "true"
      type: options
      options:
        - title: "Yes! Enable Email Autoconfirm"
          value: "true"
        - title: "No! Disable Email Autoconfirm"
          value: "false"
    ENABLE_PHONE_SIGNUP:
      title: "Enable Phone Signup"
      description: "i.e. true"
      default: "true"
      type: options
      options:
        - title: "Yes! Enable Phone Signup"
          value: "true"
        - title: "No! Disable Phone Signup"
          value: "false"
    ENABLE_PHONE_AUTOCONFIRM:
      title: "Enable Phone Autoconfirm"
      description: "i.e. true"
      default: "true"
      type: options
      options:
        - title: "Yes! Enable Phone Autoconfirm"
          value: "true"
        - title: "No! Disable Phone Autoconfirm"
          value: "false"
    SMTP_ADMIN_EMAIL:
      title: "SMTP Admin Email"
      description: "SMTP Admin Email"
      default: "admin@example.com"
      type: text
    SMTP_HOST:
      title: "SMTP Host"
      description: "SMTP Host"
      default: "supabase-mail"
      type: text
    SMTP_PORT:
      title: "SMTP Port"
      description: "SMTP Port"
      default: "2500"
      type: text
    SMTP_USER:
      title: "SMTP User"
      description: "SMTP User"
      default: "fake_mail_user"
      type: text
    SMTP_PASS:
      title: "SMTP Pass"
      description: "SMTP Pass"
      default: "fake_mail_password"
      type: text
    SMTP_SENDER_NAME:
      title: "SMTP Sender Name"
      description: "SMTP Sender Name"
      default: "fake_sender"
      type: text
    ENABLE_ANONYMOUS_USERS:
      title: "Enable Anonymous Users"
      description: "i.e. true"
      default: "true"
      type: options
      options:
        - title: "Yes! Enable Anonymous Users"
          value: "true"
        - title: "No! Disable Anonymous Users"
          value: "false"

services:
  "{{STACK_NAME}}_auth":
    environment:
      API_EXTERNAL_URL: "{{GOTRUE_API_EXTERNAL_URL}}"
      GOTRUE_API_HOST: "0.0.0.0"
      GOTRUE_API_PORT: "9999"
      GOTRUE_DB_DATABASE_URL: postgres://supabase_auth_admin:{{RANDOM_POSTGRES_PASSWORD}}@{{STACK_NAME}}_db:5432/postgres
      GOTRUE_DB_DRIVER: postgres
      GOTRUE_DISABLE_SIGNUP: "{{GOTRUE_DISABLE_SIGNUP}}"
      GOTRUE_EXTERNAL_ANONYMOUS_USERS_ENABLED: "{{ENABLE_ANONYMOUS_USERS}}"
      GOTRUE_EXTERNAL_EMAIL_ENABLED: "{{ENABLE_EMAIL_SIGNUP}}"
      GOTRUE_EXTERNAL_PHONE_ENABLED: "{{ENABLE_PHONE_SIGNUP}}"
      GOTRUE_JWT_ADMIN_ROLES: service_role
      GOTRUE_JWT_AUD: authenticated
      GOTRUE_JWT_DEFAULT_GROUP_NAME: authenticated
      GOTRUE_JWT_EXP: "3600"
      GOTRUE_JWT_SECRET: "{{JWT_SECRET}}"
      GOTRUE_MAILER_AUTOCONFIRM: "{{ENABLE_EMAIL_AUTOCONFIRM}}"
      GOTRUE_MAILER_URLPATHS_CONFIRMATION: "{{GOTRUE_MAILER_URLPATHS_CONFIRMATION}}"
      GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE: "{{GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE}}"
      GOTRUE_MAILER_URLPATHS_INVITE: "{{GOTRUE_MAILER_URLPATHS_INVITE}}"
      GOTRUE_MAILER_URLPATHS_RECOVERY: "{{GOTRUE_MAILER_URLPATHS_RECOVERY}}"
      GOTRUE_SITE_URL: "{{GOTRUE_SITE_URL}}"
      GOTRUE_SMS_AUTOCONFIRM: "{{ENABLE_PHONE_AUTOCONFIRM}}"
      GOTRUE_SMTP_ADMIN_EMAIL: "{{SMTP_ADMIN_EMAIL}}"
      GOTRUE_SMTP_HOST: "{{SMTP_HOST}}"
      GOTRUE_SMTP_PASS: "{{SMTP_PASS}}"
      GOTRUE_SMTP_PORT: "{{SMTP_PORT}}"
      GOTRUE_SMTP_SENDER_NAME: "{{SMTP_SENDER_NAME}}"
      GOTRUE_SMTP_USER: "{{SMTP_USER}}"
      GOTRUE_URI_ALLOW_LIST: ""
    custom_health_check:
      enabled: true
      test_command: "wget --no-verbose --tries=1 --spider http://localhost:9999/health"
      interval_seconds: 5
      timeout_seconds: 5
      start_period_seconds: 0
      start_interval_seconds: 0
      retries: 3
    image: supabase/gotrue:v2.151.0
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_db":
    command:
      - docker-entrypoint.sh
      - postgres
      - -c
      - config_file=/etc/postgresql/postgresql.conf
      - -c
      - log_min_messages=fatal
    environment:
      JWT_EXP: "3600"
      JWT_SECRET: "{{JWT_SECRET}}"
      PGDATABASE: postgres
      PGPASSWORD: "{{RANDOM_POSTGRES_PASSWORD}}"
      PGPORT: "5432"
      POSTGRES_DB: "postgres"
      POSTGRES_HOST: /var/run/postgresql
      POSTGRES_PASSWORD: "{{RANDOM_POSTGRES_PASSWORD}}"
      POSTGRES_PORT: "5432"
    custom_health_check:
      enabled: true
      test_command: "pg_isready -U postgres -h localhost"
      interval_seconds: 5
      timeout_seconds: 5
      start_period_seconds: 0
      start_interval_seconds: 0
      retries: 10
    image: supabase/postgres:15.1.1.61
    configs:
      - content: |
          \set pguser `echo "$POSTGRES_USER"`
          create schema if not exists _realtime;
          alter schema _realtime owner to :pguser;

        uid: 0
        gid: 0
        mounting_path: "/docker-entrypoint-initdb.d/migrations/99-realtime.sql"
      - content: |
          BEGIN;
            -- Create pg_net extension
            CREATE EXTENSION IF NOT EXISTS pg_net SCHEMA extensions;
            -- Create supabase_functions schema
            CREATE SCHEMA supabase_functions AUTHORIZATION supabase_admin;
            GRANT USAGE ON SCHEMA supabase_functions TO postgres, anon, authenticated, service_role;
            ALTER DEFAULT PRIVILEGES IN SCHEMA supabase_functions GRANT ALL ON TABLES TO postgres, anon, authenticated, service_role;
            ALTER DEFAULT PRIVILEGES IN SCHEMA supabase_functions GRANT ALL ON FUNCTIONS TO postgres, anon, authenticated, service_role;
            ALTER DEFAULT PRIVILEGES IN SCHEMA supabase_functions GRANT ALL ON SEQUENCES TO postgres, anon, authenticated, service_role;
            -- supabase_functions.migrations definition
            CREATE TABLE supabase_functions.migrations (
              version text PRIMARY KEY,
              inserted_at timestamptz NOT NULL DEFAULT NOW()
            );
            -- Initial supabase_functions migration
            INSERT INTO supabase_functions.migrations (version) VALUES ('initial');
            -- supabase_functions.hooks definition
            CREATE TABLE supabase_functions.hooks (
              id bigserial PRIMARY KEY,
              hook_table_id integer NOT NULL,
              hook_name text NOT NULL,
              created_at timestamptz NOT NULL DEFAULT NOW(),
              request_id bigint
            );
            CREATE INDEX supabase_functions_hooks_request_id_idx ON supabase_functions.hooks USING btree (request_id);
            CREATE INDEX supabase_functions_hooks_h_table_id_h_name_idx ON supabase_functions.hooks USING btree (hook_table_id, hook_name);
            COMMENT ON TABLE supabase_functions.hooks IS 'Supabase Functions Hooks: Audit trail for triggered hooks.';
            CREATE FUNCTION supabase_functions.http_request()
              RETURNS trigger
              LANGUAGE plpgsql
              AS $function$
              DECLARE
                request_id bigint;
                payload jsonb;
                url text := TG_ARGV[0]::text;
                method text := TG_ARGV[1]::text;
                headers jsonb DEFAULT '{}'::jsonb;
                params jsonb DEFAULT '{}'::jsonb;
                timeout_ms integer DEFAULT 1000;
              BEGIN
                IF url IS NULL OR url = 'null' THEN
                  RAISE EXCEPTION 'url argument is missing';
                END IF;

                IF method IS NULL OR method = 'null' THEN
                  RAISE EXCEPTION 'method argument is missing';
                END IF;

                IF TG_ARGV[2] IS NULL OR TG_ARGV[2] = 'null' THEN
                  headers = '{"Content-Type": "application/json"}'::jsonb;
                ELSE
                  headers = TG_ARGV[2]::jsonb;
                END IF;

                IF TG_ARGV[3] IS NULL OR TG_ARGV[3] = 'null' THEN
                  params = '{}'::jsonb;
                ELSE
                  params = TG_ARGV[3]::jsonb;
                END IF;

                IF TG_ARGV[4] IS NULL OR TG_ARGV[4] = 'null' THEN
                  timeout_ms = 1000;
                ELSE
                  timeout_ms = TG_ARGV[4]::integer;
                END IF;

                CASE
                  WHEN method = 'GET' THEN
                    SELECT http_get INTO request_id FROM net.http_get(
                      url,
                      params,
                      headers,
                      timeout_ms
                    );
                  WHEN method = 'POST' THEN
                    payload = jsonb_build_object(
                      'old_record', OLD,
                      'record', NEW,
                      'type', TG_OP,
                      'table', TG_TABLE_NAME,
                      'schema', TG_TABLE_SCHEMA
                    );

                    SELECT http_post INTO request_id FROM net.http_post(
                      url,
                      payload,
                      params,
                      headers,
                      timeout_ms
                    );
                  ELSE
                    RAISE EXCEPTION 'method argument % is invalid', method;
                END CASE;

                INSERT INTO supabase_functions.hooks
                  (hook_table_id, hook_name, request_id)
                VALUES
                  (TG_RELID, TG_NAME, request_id);

                RETURN NEW;
              END
            $function$;
            -- Supabase super admin
            DO
            $$
            BEGIN
              IF NOT EXISTS (
                SELECT 1
                FROM pg_roles
                WHERE rolname = 'supabase_functions_admin'
              )
              THEN
                CREATE USER supabase_functions_admin NOINHERIT CREATEROLE LOGIN NOREPLICATION;
              END IF;
            END
            $$;
            GRANT ALL PRIVILEGES ON SCHEMA supabase_functions TO supabase_functions_admin;
            GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA supabase_functions TO supabase_functions_admin;
            GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA supabase_functions TO supabase_functions_admin;
            ALTER USER supabase_functions_admin SET search_path = "supabase_functions";
            ALTER table "supabase_functions".migrations OWNER TO supabase_functions_admin;
            ALTER table "supabase_functions".hooks OWNER TO supabase_functions_admin;
            ALTER function "supabase_functions".http_request() OWNER TO supabase_functions_admin;
            GRANT supabase_functions_admin TO postgres;
            -- Remove unused supabase_pg_net_admin role
            DO
            $$
            BEGIN
              IF EXISTS (
                SELECT 1
                FROM pg_roles
                WHERE rolname = 'supabase_pg_net_admin'
              )
              THEN
                REASSIGN OWNED BY supabase_pg_net_admin TO supabase_admin;
                DROP OWNED BY supabase_pg_net_admin;
                DROP ROLE supabase_pg_net_admin;
              END IF;
            END
            $$;
            -- pg_net grants when extension is already enabled
            DO
            $$
            BEGIN
              IF EXISTS (
                SELECT 1
                FROM pg_extension
                WHERE extname = 'pg_net'
              )
              THEN
                GRANT USAGE ON SCHEMA net TO supabase_functions_admin, postgres, anon, authenticated, service_role;
                ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;
                ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;
                ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;
                ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;
                REVOKE ALL ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;
                REVOKE ALL ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;
                GRANT EXECUTE ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
                GRANT EXECUTE ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
              END IF;
            END
            $$;
            -- Event trigger for pg_net
            CREATE OR REPLACE FUNCTION extensions.grant_pg_net_access()
            RETURNS event_trigger
            LANGUAGE plpgsql
            AS $$
            BEGIN
              IF EXISTS (
                SELECT 1
                FROM pg_event_trigger_ddl_commands() AS ev
                JOIN pg_extension AS ext
                ON ev.objid = ext.oid
                WHERE ext.extname = 'pg_net'
              )
              THEN
                GRANT USAGE ON SCHEMA net TO supabase_functions_admin, postgres, anon, authenticated, service_role;
                ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;
                ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SECURITY DEFINER;
                ALTER function net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;
                ALTER function net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) SET search_path = net;
                REVOKE ALL ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;
                REVOKE ALL ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) FROM PUBLIC;
                GRANT EXECUTE ON FUNCTION net.http_get(url text, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
                GRANT EXECUTE ON FUNCTION net.http_post(url text, body jsonb, params jsonb, headers jsonb, timeout_milliseconds integer) TO supabase_functions_admin, postgres, anon, authenticated, service_role;
              END IF;
            END;
            $$;
            COMMENT ON FUNCTION extensions.grant_pg_net_access IS 'Grants access to pg_net';
            DO
            $$
            BEGIN
              IF NOT EXISTS (
                SELECT 1
                FROM pg_event_trigger
                WHERE evtname = 'issue_pg_net_access'
              ) THEN
                CREATE EVENT TRIGGER issue_pg_net_access ON ddl_command_end WHEN TAG IN ('CREATE EXTENSION')
                EXECUTE PROCEDURE extensions.grant_pg_net_access();
              END IF;
            END
            $$;
            INSERT INTO supabase_functions.migrations (version) VALUES ('20210809183423_update_grants');
            ALTER function supabase_functions.http_request() SECURITY DEFINER;
            ALTER function supabase_functions.http_request() SET search_path = supabase_functions;
            REVOKE ALL ON FUNCTION supabase_functions.http_request() FROM PUBLIC;
            GRANT EXECUTE ON FUNCTION supabase_functions.http_request() TO postgres, anon, authenticated, service_role;
          COMMIT;

        uid: 0
        gid: 0
        mounting_path: "/docker-entrypoint-initdb.d/init-scripts/98-webhooks.sql"
      - content: |
          -- NOTE: change to your own passwords for production environments
          \set pgpass `echo "$POSTGRES_PASSWORD"`

          ALTER USER authenticator WITH PASSWORD :'pgpass';
          ALTER USER pgbouncer WITH PASSWORD :'pgpass';
          ALTER USER supabase_auth_admin WITH PASSWORD :'pgpass';
          ALTER USER supabase_functions_admin WITH PASSWORD :'pgpass';
          ALTER USER supabase_storage_admin WITH PASSWORD :'pgpass';

        uid: 0
        gid: 0
        mounting_path: "/docker-entrypoint-initdb.d/init-scripts/99-roles.sql"
      - content: |
          \set jwt_secret `echo "$JWT_SECRET"`
          \set jwt_exp `echo "$JWT_EXP"`

          ALTER DATABASE postgres SET "app.settings.jwt_secret" TO :'jwt_secret';
          ALTER DATABASE postgres SET "app.settings.jwt_exp" TO :'jwt_exp';

        uid: 0
        gid: 0
        mounting_path: "/docker-entrypoint-initdb.d/init-scripts/99-jwt.sql"
      - content: |
          \set pguser `echo "$POSTGRES_USER"`

          create schema if not exists _analytics;
          alter schema _analytics owner to :pguser;

        uid: 0
        gid: 0
        mounting_path: "/docker-entrypoint-initdb.d/migrations/99-logs.sql"
    volumes:
      - "{{POSTGRES_DATA_STORAGE}}:/var/lib/postgresql/data"
      - "{{POSTGRES_CONFIG}}:/etc/postgresql-custom"
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_functions":
    command:
      - edge-runtime
      - start
      - --main-service
      - /home/deno/functions/main
    environment:
      JWT_SECRET: "{{JWT_SECRET}}"
      SUPABASE_ANON_KEY: "{{ANON_JWT_KEY}}"
      SUPABASE_DB_URL: postgresql://postgres:{{RANDOM_POSTGRES_PASSWORD}}@{{STACK_NAME}}_db:5432/postgres
      SUPABASE_SERVICE_ROLE_KEY: "{{SERVICE_ROLE_JWT_KEY}}"
      SUPABASE_URL: http://{{STACK_NAME}}_kong:8000
      VERIFY_JWT: "{{FUNCTIONS_VERIFY_JWT}}"
    configs:
      - content: |
          import { serve } from 'https://deno.land/std@0.131.0/http/server.ts'
          import * as jose from 'https://deno.land/x/jose@v4.14.4/index.ts'

          console.log('main function started')

          const JWT_SECRET = Deno.env.get('JWT_SECRET')
          const VERIFY_JWT = Deno.env.get('VERIFY_JWT') === 'true'

          function getAuthToken(req: Request) {
            const authHeader = req.headers.get('authorization')
            if (!authHeader) {
              throw new Error('Missing authorization header')
            }
            const [bearer, token] = authHeader.split(' ')
            if (bearer !== 'Bearer') {
              throw new Error(`Auth header is not 'Bearer {token}'`)
            }
            return token
          }

          async function verifyJWT(jwt: string): Promise<boolean> {
            const encoder = new TextEncoder()
            const secretKey = encoder.encode(JWT_SECRET)
            try {
              await jose.jwtVerify(jwt, secretKey)
            } catch (err) {
              console.error(err)
              return false
            }
            return true
          }

          serve(async (req: Request) => {
            if (req.method !== 'OPTIONS' && VERIFY_JWT) {
              try {
                const token = getAuthToken(req)
                const isValidJWT = await verifyJWT(token)

                if (!isValidJWT) {
                  return new Response(JSON.stringify({ msg: 'Invalid JWT' }), {
                    status: 401,
                    headers: { 'Content-Type': 'application/json' },
                  })
                }
              } catch (e) {
                console.error(e)
                return new Response(JSON.stringify({ msg: e.toString() }), {
                  status: 401,
                  headers: { 'Content-Type': 'application/json' },
                })
              }
            }

            const url = new URL(req.url)
            const { pathname } = url
            const path_parts = pathname.split('/')
            const service_name = path_parts[1]

            if (!service_name || service_name === '') {
              const error = { msg: 'missing function name in request' }
              return new Response(JSON.stringify(error), {
                status: 400,
                headers: { 'Content-Type': 'application/json' },
              })
            }

            const servicePath = `/home/deno/functions/${service_name}`
            console.error(`serving the request with ${servicePath}`)

            const memoryLimitMb = 150
            const workerTimeoutMs = 1 * 60 * 1000
            const noModuleCache = false
            const importMapPath = null
            const envVarsObj = Deno.env.toObject()
            const envVars = Object.keys(envVarsObj).map((k) => [k, envVarsObj[k]])

            try {
              const worker = await EdgeRuntime.userWorkers.create({
                servicePath,
                memoryLimitMb,
                workerTimeoutMs,
                noModuleCache,
                importMapPath,
                envVars,
              })
              return await worker.fetch(req)
            } catch (e) {
              const error = { msg: e.toString() }
              return new Response(JSON.stringify(error), {
                status: 500,
                headers: { 'Content-Type': 'application/json' },
              })
            }
          })

        uid: 0
        gid: 0
        mounting_path: "/home/deno/functions/main/index.ts"

      - content: |
          // Follow this setup guide to integrate the Deno language server with your editor:
          // https://deno.land/manual/getting_started/setup_your_environment
          // This enables autocomplete, go to definition, etc.

          import { serve } from "https://deno.land/std@0.177.1/http/server.ts"

          serve(async () => {
            return new Response(
              `"Hello from Edge Functions!"`,
              { headers: { "Content-Type": "application/json" } },
            )
          })

          // To invoke:
          // curl 'http://localhost:<KONG_HTTP_PORT>/functions/v1/hello' \
          //   --header 'Authorization: Bearer <anon/service_role API key>'

        uid: 0
        gid: 0
        mounting_path: "/home/deno/functions/hello/index.ts"
    image: supabase/edge-runtime:v1.53.3
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"
    volumes:
      - "{{FUNCTIONS_STORAGE}}:/home/deno/functions"

  "{{STACK_NAME}}_imgproxy":
    environment:
      IMGPROXY_BIND: ":5001"
      IMGPROXY_ENABLE_WEBP_DETECTION: "true"
      IMGPROXY_LOCAL_FILESYSTEM_ROOT: /
      IMGPROXY_USE_ETAG: "true"
    custom_health_check:
      enabled: true
      test_command: "imgproxy health"
      interval_seconds: 5
      timeout_seconds: 5
      start_period_seconds: 0
      start_interval_seconds: 0
      retries: 3
    image: darthsim/imgproxy:v3.8.0
    volumes:
      - "{{IMGPROXY_STORAGE}}:/var/lib/storage"
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_kong":
    expose:
      - "8000/http/Supabase Dashboard"
    command:
      - sh /app-entrypoint.sh
    environment:
      DASHBOARD_PASSWORD: "{{DASHBOARD_PASSWORD}}"
      DASHBOARD_USERNAME: "{{DASHBOARD_USERNAME}}"
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /home/kong/kong.yml
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: "160k"
      KONG_NGINX_PROXY_PROXY_BUFFERS: "64 160k"
      KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
      SUPABASE_ANON_KEY: "{{ANON_JWT_KEY}}"
      SUPABASE_SERVICE_KEY: "{{SERVICE_ROLE_JWT_KEY}}"
    image: kong:2.8.1
    configs:
      - content: |
          set -e
          eval "echo \"$(cat ~/temp.yml)\"" > ~/kong.yml
          /docker-entrypoint.sh kong docker-start
        uid: 0
        gid: 0
        mode: 0755
        mounting_path: "/app-entrypoint.sh"
      - content: |
          _format_version: '2.1'
          _transform: true

          ###
          ### Consumers / Users
          ###
          consumers:
            - username: DASHBOARD
            - username: anon
              keyauth_credentials:
                - key: $SUPABASE_ANON_KEY
            - username: service_role
              keyauth_credentials:
                - key: $SUPABASE_SERVICE_KEY

          ###
          ### Access Control List
          ###
          acls:
            - consumer: anon
              group: anon
            - consumer: service_role
              group: admin

          ###
          ### Dashboard credentials
          ###
          basicauth_credentials:
            - consumer: DASHBOARD
              username: $DASHBOARD_USERNAME
              password: $DASHBOARD_PASSWORD

          ###
          ### API Routes
          ###
          services:
            ## Open Auth routes
            - name: auth-v1-open
              url: http://{{STACK_NAME}}_auth:9999/verify
              routes:
                - name: auth-v1-open
                  strip_path: true
                  paths:
                    - /auth/v1/verify
              plugins:
                - name: cors
            - name: auth-v1-open-callback
              url: http://{{STACK_NAME}}_auth:9999/callback
              routes:
                - name: auth-v1-open-callback
                  strip_path: true
                  paths:
                    - /auth/v1/callback
              plugins:
                - name: cors
            - name: auth-v1-open-authorize
              url: http://{{STACK_NAME}}_auth:9999/authorize
              routes:
                - name: auth-v1-open-authorize
                  strip_path: true
                  paths:
                    - /auth/v1/authorize
              plugins:
                - name: cors

            ## Secure Auth routes
            - name: auth-v1
              url: http://{{STACK_NAME}}_auth:9999/
              routes:
                - name: auth-v1-all
                  strip_path: true
                  paths:
                    - /auth/v1/
              plugins:
                - name: cors
                - name: key-auth
                  config:
                    hide_credentials: false
                - name: acl
                  config:
                    hide_groups_header: true
                    allow:
                      - admin
                      - anon

            ## Secure REST routes
            - name: rest-v1
              url: http://{{STACK_NAME}}_rest:3000/
              routes:
                - name: rest-v1-all
                  strip_path: true
                  paths:
                    - /rest/v1/
              plugins:
                - name: cors
                - name: key-auth
                  config:
                    hide_credentials: true
                - name: acl
                  config:
                    hide_groups_header: true
                    allow:
                      - admin
                      - anon

            ## Secure GraphQL routes
            - name: graphql-v1
              url: http://{{STACK_NAME}}_rest:3000/rpc/graphql
              routes:
                - name: graphql-v1-all
                  strip_path: true
                  paths:
                    - /graphql/v1
              plugins:
                - name: cors
                - name: key-auth
                  config:
                    hide_credentials: true
                - name: request-transformer
                  config:
                    add:
                      headers:
                        - Content-Profile:graphql_public
                - name: acl
                  config:
                    hide_groups_header: true
                    allow:
                      - admin
                      - anon

            ## Secure Realtime routes
            - name: realtime-v1-ws
              url: http://{{STACK_NAME}}_realtime_dev:4000/socket
              protocol: ws
              routes:
                - name: realtime-v1-ws
                  strip_path: true
                  paths:
                    - /realtime/v1/
              plugins:
                - name: cors
                - name: key-auth
                  config:
                    hide_credentials: false
                - name: acl
                  config:
                    hide_groups_header: true
                    allow:
                      - admin
                      - anon

            - name: realtime-v1-rest
              url: http://{{STACK_NAME}}_realtime_dev:4000/api
              protocol: http
              routes:
                - name: realtime-v1-rest
                  strip_path: true
                  paths:
                    - /realtime/v1/api
              plugins:
                - name: cors
                - name: key-auth
                  config:
                    hide_credentials: false
                - name: acl
                  config:
                    hide_groups_header: true
                    allow:
                      - admin
                      - anon
            ## Storage routes: the storage server manages its own auth
            - name: storage-v1
              url: http://{{STACK_NAME}}_storage:5000/
              routes:
                - name: storage-v1-all
                  strip_path: true
                  paths:
                    - /storage/v1/
              plugins:
                - name: cors

            ## Edge Functions routes
            - name: functions-v1
              url: http://{{STACK_NAME}}_functions:9000/
              routes:
                - name: functions-v1-all
                  strip_path: true
                  paths:
                    - /functions/v1/
              plugins:
                - name: cors

            ## Analytics routes
            - name: analytics-v1
              url: http://{{STACK_NAME}}_analytics:4000/
              routes:
                - name: analytics-v1-all
                  strip_path: true
                  paths:
                    - /analytics/v1/

            ## Secure Database routes
            - name: meta
              url: http://{{STACK_NAME}}_meta:8080/
              routes:
                - name: meta-all
                  strip_path: true
                  paths:
                    - /pg/
              plugins:
                - name: key-auth
                  config:
                    hide_credentials: false
                - name: acl
                  config:
                    hide_groups_header: true
                    allow:
                      - admin

            ## Protected Dashboard - catch all remaining routes
            - name: dashboard
              url: http://{{STACK_NAME}}_studio:3000/
              routes:
                - name: dashboard-all
                  strip_path: true
                  paths:
                    - /
              plugins:
                - name: cors
                - name: basic-auth
                  config:
                    hide_credentials: true

        uid: 0
        gid: 0
        mounting_path: "/home/kong/temp.yml"
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_meta":
    environment:
      PG_META_DB_HOST: "{{STACK_NAME}}_db"
      PG_META_DB_NAME: "postgres"
      PG_META_DB_PASSWORD: "{{RANDOM_POSTGRES_PASSWORD}}"
      PG_META_DB_PORT: "5432"
      PG_META_DB_USER: supabase_admin
      PG_META_PORT: "8080"
    image: supabase/postgres-meta:v0.80.0
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_realtime_dev":
    command:
      - /usr/bin/tini -s -g -- sh /app-entrypoint.sh
    environment:
      API_JWT_SECRET: "{{JWT_SECRET}}"
      DB_AFTER_CONNECT_QUERY: SET search_path TO _realtime
      DB_ENC_KEY: supabaserealtime
      DB_HOST: "{{STACK_NAME}}_db"
      DB_NAME: postgres
      DB_PASSWORD: "{{RANDOM_POSTGRES_PASSWORD}}"
      DB_PORT: "5432"
      DB_USER: supabase_admin
      DNS_NODES: "''"
      ENABLE_TAILSCALE: "false"
      ERL_AFLAGS: -proto_dist inet_tcp
      FLY_ALLOC_ID: fly123
      FLY_APP_NAME: realtime
      PORT: "4000"
      SECRET_KEY_BASE: "{{RANDOM_SECRET_KEY_BASE}}"
    custom_health_check:
      enabled: true
      test_command: "curl -sSfL --head -o /dev/null -H 'Authorization: Bearer {{ANON_JWT_KEY}}' http://localhost:4000/api/tenants/{{STACK_NAME}}_realtime_dev/health"
      interval_seconds: 5
      timeout_seconds: 5
      start_period_seconds: 0
      start_interval_seconds: 0
      retries: 3
    image: supabase/realtime:v2.28.32
    configs:
      - content: |
          /app/bin/migrate
          /app/bin/realtime eval 'Realtime.Release.seeds(Realtime.Repo)'
          /app/bin/server
        uid: 0
        gid: 0
        mode: 0755
        mounting_path: "/app-entrypoint.sh"
      - content: |
          alias Realtime.{Api.Tenant, Repo}
          import Ecto.Adapters.SQL, only: [query: 3]

          tenant_name = "{{STACK_NAME}}_realtime_dev"

          env = if :ets.whereis(Mix.State) != :undefined, do: Mix.env(), else: :prod
          default_db_host = if env in [:dev, :test], do: "localhost", else: "host.docker.internal"

          Repo.transaction(fn ->
            case Repo.get_by(Tenant, external_id: tenant_name) do
              %Tenant{} = tenant -> Repo.delete!(tenant)
              nil -> {:ok, nil}
            end

            %Tenant{}
            |> Tenant.changeset(%{
              "name" => tenant_name,
              "external_id" => tenant_name,
              "jwt_secret" =>
                System.get_env("API_JWT_SECRET", "super-secret-jwt-token-with-at-least-32-characters-long"),
              "extensions" => [
                %{
                  "type" => "postgres_cdc_rls",
                  "settings" => %{
                    "db_name" => System.get_env("DB_NAME", "postgres"),
                    "db_host" => System.get_env("DB_HOST", default_db_host),
                    "db_user" => System.get_env("DB_USER", "supabase_admin"),
                    "db_password" => System.get_env("DB_PASSWORD", "postgres"),
                    "db_port" => System.get_env("DB_PORT", "5432"),
                    "region" => "us-east-1",
                    "poll_interval_ms" => 100,
                    "poll_max_record_bytes" => 1_048_576,
                    "ssl_enforced" => false
                  }
                }
              ]
            })
            |> Repo.insert!()
          end)

          if env in [:dev, :test] do
            publication = "supabase_realtime"

            {:ok, _} =
              Repo.transaction(fn ->
                [
                  "drop publication if exists #{publication}",
                  "drop table if exists public.test_tenant;",
                  "create table public.test_tenant ( id SERIAL PRIMARY KEY, details text );",
                  "grant all on table public.test_tenant to anon;",
                  "grant all on table public.test_tenant to postgres;",
                  "grant all on table public.test_tenant to authenticated;",
                  "create publication #{publication} for table public.test_tenant"
                ]
                |> Enum.each(&query(Repo, &1, []))
              end)
          end
        uid: 0
        gid: 0
        mounting_path: "/app/lib/realtime-2.28.32/priv/repo/seeds.exs"

    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_rest":
    command:
      - postgrest
    environment:
      PGRST_APP_SETTINGS_JWT_EXP: "3600"
      PGRST_APP_SETTINGS_JWT_SECRET: "{{JWT_SECRET}}"
      PGRST_DB_ANON_ROLE: anon
      PGRST_DB_SCHEMAS: public,storage,graphql_public
      PGRST_DB_URI: postgres://authenticator:{{RANDOM_POSTGRES_PASSWORD}}@{{STACK_NAME}}_db:5432/postgres
      PGRST_DB_USE_LEGACY_GUCS: "false"
      PGRST_JWT_SECRET: "{{JWT_SECRET}}"
    image: postgrest/postgrest:v12.0.1
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_storage":
    environment:
      ANON_KEY: "{{ANON_JWT_KEY}}"
      DATABASE_URL: postgres://supabase_storage_admin:{{RANDOM_POSTGRES_PASSWORD}}@{{STACK_NAME}}_db:5432/postgres
      ENABLE_IMAGE_TRANSFORMATION: "true"
      FILE_SIZE_LIMIT: "52428800"
      FILE_STORAGE_BACKEND_PATH: /var/lib/storage
      GLOBAL_S3_BUCKET: stub
      IMGPROXY_URL: http://{{STACK_NAME}}_imgproxy:5001
      PGRST_JWT_SECRET: "{{JWT_SECRET}}"
      POSTGREST_URL: http://{{STACK_NAME}}_rest:3000
      REGION: stub
      SERVICE_KEY: "{{SERVICE_ROLE_JWT_KEY}}"
      STORAGE_BACKEND: file
      TENANT_ID: stub
    custom_health_check:
      enabled: true
      test_command: "wget --no-verbose --tries=1 --spider http://0.0.0.0:5000/status"
      interval_seconds: 5
      timeout_seconds: 5
      start_period_seconds: 0
      start_interval_seconds: 0
      retries: 3
    image: supabase/storage-api:v1.0.6
    volumes:
      - "{{SUPABASE_BUCKET_STORAGE}}:/var/lib/storage"
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_studio":
    environment:
      AUTH_JWT_SECRET: "{{JWT_SECRET}}"
      DEFAULT_ORGANIZATION_NAME: "{{STUDIO_DEFAULT_ORGANIZATION_NAME}}"
      DEFAULT_PROJECT_NAME: "{{STUDIO_DEFAULT_PROJECT_NAME}}"
      LOGFLARE_API_KEY: "{{RANDOM_LOGFLARE_API_KEY}}"
      LOGFLARE_URL: http://{{STACK_NAME}}_analytics:4000
      NEXT_ANALYTICS_BACKEND_PROVIDER: postgres
      NEXT_PUBLIC_ENABLE_LOGS: "true"
      POSTGRES_PASSWORD: "{{RANDOM_POSTGRES_PASSWORD}}"
      STUDIO_PG_META_URL: http://{{STACK_NAME}}_meta:8080
      SUPABASE_ANON_KEY: "{{ANON_JWT_KEY}}"
      SUPABASE_PUBLIC_URL: "{{SUPABASE_PUBLIC_URL}}"
      SUPABASE_SERVICE_KEY: "{{SERVICE_ROLE_JWT_KEY}}"
      SUPABASE_URL: http://{{STACK_NAME}}_kong:8000
      HOSTNAME: "0.0.0.0"
    custom_health_check:
      enabled: true
      test_command: 'node -e "require(''http'').get(''http://0.0.0.0:3000/api/profile'', (r) => {if (r.statusCode !== 200) throw new Error(r.statusCode)})"'
      interval_seconds: 5
      timeout_seconds: 15
      start_period_seconds: 0
      start_interval_seconds: 0
      retries: 3
    image: supabase/studio:20240422-5cf8f30
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_analytics":
    image: supabase/logflare:1.4.0
    custom_health_check:
      enabled: true
      test_command: "curl http://0.0.0.0:4000/health"
      interval_seconds: 5
      timeout_seconds: 5
      start_period_seconds: 0
      start_interval_seconds: 0
      retries: 10
    environment:
      DB_USERNAME: supabase_admin
      DB_DATABASE: postgres
      DB_HOSTNAME: "{{STACK_NAME}}_db"
      DB_PORT: "5432"
      DB_PASSWORD: "{{RANDOM_POSTGRES_PASSWORD}}"
      DB_SCHEMA: _analytics
      LOGFLARE_API_KEY: "{{RANDOM_LOGFLARE_API_KEY}}"
      LOGFLARE_SINGLE_TENANT: "true"
      LOGFLARE_SUPABASE_MODE: "true"
      LOGFLARE_MIN_CLUSTER_SIZE: "1"
      LOGFLARE_NODE_HOST: "127.0.0.1"

      # Comment variables to use Big Query backend for analytics
      POSTGRES_BACKEND_URL: postgresql://supabase_admin:{{RANDOM_POSTGRES_PASSWORD}}@{{STACK_NAME}}_db:5432/postgres
      POSTGRES_BACKEND_SCHEMA: _analytics
      LOGFLARE_FEATURE_FLAG_OVERRIDE: multibackend=true
      # Uncomment to use Big Query backend for analytics
      # GOOGLE_PROJECT_ID: ${GOOGLE_PROJECT_ID}
      # GOOGLE_PROJECT_NUMBER: ${GOOGLE_PROJECT_NUMBER}
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"

  "{{STACK_NAME}}_vector":
    image: timberio/vector:0.28.1-alpine
    custom_health_check:
      enabled: true
      test_command: "wget --no-verbose --tries=1 --spider http://0.0.0.0:9001/health"
      interval_seconds: 5
      timeout_seconds: 5
      start_period_seconds: 0
      start_interval_seconds: 0
      retries: 3
    configs:
      - content: |
          api:
            enabled: true
            address: 0.0.0.0:9001

          sources:
            docker_host:
              type: docker_logs
              exclude_containers:
                - {{STACK_NAME}}_vector
              include_containers:
                - {{STACK_NAME}}_kong
                - {{STACK_NAME}}_auth
                - {{STACK_NAME}}_rest
                - {{STACK_NAME}}_realtime_dev
                - {{STACK_NAME}}_storage
                - {{STACK_NAME}}_functions
                - {{STACK_NAME}}_db

          transforms:
            project_logs:
              type: remap
              inputs:
                - docker_host
              source: |-
                .project = "default"
                .event_message = del(.message)
                .appname = parse_regex!(.container_name, r'^(?P<container_name>[^\.]+)\.\d+\.[^\.]+').container_name
                del(.container_name)
                del(.container_created_at)
                del(.container_id)
                del(.source_type)
                del(.stream)
                del(.label)
                del(.image)
                del(.host)
                del(.stream)
            router:
              type: route
              inputs:
                - project_logs
              route:
                kong: '.appname == "{{STACK_NAME}}_kong"'
                auth: '.appname == "{{STACK_NAME}}_auth"'
                rest: '.appname == "{{STACK_NAME}}_rest"'
                realtime: '.appname == "{{STACK_NAME}}_realtime_dev"'
                storage: '.appname == "{{STACK_NAME}}_storage"'
                functions: '.appname == "{{STACK_NAME}}_functions"'
                db: '.appname == "{{STACK_NAME}}_db"'
            # Ignores non nginx errors since they are related with kong booting up
            kong_logs:
              type: remap
              inputs:
                - router.kong
              source: |-
                req, err = parse_nginx_log(.event_message, "combined")
                if err == null {
                    .timestamp = req.timestamp
                    .metadata.request.headers.referer = req.referer
                    .metadata.request.headers.user_agent = req.agent
                    .metadata.request.headers.cf_connecting_ip = req.client
                    .metadata.request.method = req.method
                    .metadata.request.path = req.path
                    .metadata.request.protocol = req.protocol
                    .metadata.response.status_code = req.status
                }
                if err != null {
                  abort
                }
            # Ignores non nginx errors since they are related with kong booting up
            kong_err:
              type: remap
              inputs:
                - router.kong
              source: |-
                .metadata.request.method = "GET"
                .metadata.response.status_code = 200
                parsed, err = parse_nginx_log(.event_message, "error")
                if err == null {
                    .timestamp = parsed.timestamp
                    .severity = parsed.severity
                    .metadata.request.host = parsed.host
                    .metadata.request.headers.cf_connecting_ip = parsed.client
                    url, err = split(parsed.request, " ")
                    if err == null {
                        .metadata.request.method = url[0]
                        .metadata.request.path = url[1]
                        .metadata.request.protocol = url[2]
                    }
                }
                if err != null {
                  abort
                }
            # Gotrue logs are structured json strings which frontend parses directly. But we keep metadata for consistency.
            auth_logs:
              type: remap
              inputs:
                - router.auth
              source: |-
                parsed, err = parse_json(.event_message)
                if err == null {
                    .metadata.timestamp = parsed.time
                    .metadata = merge!(.metadata, parsed)
                }
            # PostgREST logs are structured so we separate timestamp from message using regex
            rest_logs:
              type: remap
              inputs:
                - router.rest
              source: |-
                parsed, err = parse_regex(.event_message, r'^(?P<time>.*): (?P<msg>.*)$')
                if err == null {
                    .event_message = parsed.msg
                    .timestamp = to_timestamp!(parsed.time)
                    .metadata.host = .project
                }
            # Realtime logs are structured so we parse the severity level using regex (ignore time because it has no date)
            realtime_logs:
              type: remap
              inputs:
                - router.realtime
              source: |-
                .metadata.project = del(.project)
                .metadata.external_id = .metadata.project
                parsed, err = parse_regex(.event_message, r'^(?P<time>\d+:\d+:\d+\.\d+) \[(?P<level>\w+)\] (?P<msg>.*)$')
                if err == null {
                    .event_message = parsed.msg
                    .metadata.level = parsed.level
                }
            # Storage logs may contain json objects so we parse them for completeness
            storage_logs:
              type: remap
              inputs:
                - router.storage
              source: |-
                .metadata.project = del(.project)
                .metadata.tenantId = .metadata.project
                parsed, err = parse_json(.event_message)
                if err == null {
                    .event_message = parsed.msg
                    .metadata.level = parsed.level
                    .metadata.timestamp = parsed.time
                    .metadata.context[0].host = parsed.hostname
                    .metadata.context[0].pid = parsed.pid
                }
            # Postgres logs some messages to stderr which we map to warning severity level
            db_logs:
              type: remap
              inputs:
                - router.db
              source: |-
                .metadata.host = "db-default"
                .metadata.parsed.timestamp = .timestamp

                parsed, err = parse_regex(.event_message, r'.*(?P<level>INFO|NOTICE|WARNING|ERROR|LOG|FATAL|PANIC?):.*', numeric_groups: true)

                if err != null || parsed == null {
                  .metadata.parsed.error_severity = "info"
                }
                if parsed != null {
                .metadata.parsed.error_severity = parsed.level
                }
                if .metadata.parsed.error_severity == "info" {
                    .metadata.parsed.error_severity = "log"
                }
                .metadata.parsed.error_severity = upcase!(.metadata.parsed.error_severity)

          sinks:
            logflare_auth:
              type: "http"
              inputs:
                - auth_logs
              encoding:
                codec: "json"
              method: "post"
              request:
                retry_max_duration_secs: 10
              uri: "http://{{STACK_NAME}}_analytics:4000/api/logs?source_name=gotrue.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}"
            logflare_realtime:
              type: "http"
              inputs:
                - realtime_logs
              encoding:
                codec: "json"
              method: "post"
              request:
                retry_max_duration_secs: 10
              uri: "http://{{STACK_NAME}}_analytics:4000/api/logs?source_name=realtime.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}"
            logflare_rest:
              type: "http"
              inputs:
                - rest_logs
              encoding:
                codec: "json"
              method: "post"
              request:
                retry_max_duration_secs: 10
              uri: "http://{{STACK_NAME}}_analytics:4000/api/logs?source_name=postgREST.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}"
            logflare_db:
              type: "http"
              inputs:
                - db_logs
              encoding:
                codec: "json"
              method: "post"
              request:
                retry_max_duration_secs: 10
              # We must route the sink through kong because ingesting logs before logflare is fully initialised will
              # lead to broken queries from studio. This works by the assumption that containers are started in the
              # following order: vector > db > logflare > kong
              uri: "http://{{STACK_NAME}}_kong:8000/analytics/v1/api/logs?source_name=postgres.logs&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}"
            logflare_functions:
              type: "http"
              inputs:
                - router.functions
              encoding:
                codec: "json"
              method: "post"
              request:
                retry_max_duration_secs: 10
              uri: "http://{{STACK_NAME}}_analytics:4000/api/logs?source_name=deno-relay-logs&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}"
            logflare_storage:
              type: "http"
              inputs:
                - storage_logs
              encoding:
                codec: "json"
              method: "post"
              request:
                retry_max_duration_secs: 10
              uri: "http://{{STACK_NAME}}_analytics:4000/api/logs?source_name=storage.logs.prod.2&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}"
            logflare_kong:
              type: "http"
              inputs:
                - kong_logs
                - kong_err
              encoding:
                codec: "json"
              method: "post"
              request:
                retry_max_duration_secs: 10
              uri: "http://{{STACK_NAME}}_analytics:4000/api/logs?source_name=cloudflare.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}"

        uid: 0
        gid: 0
        mounting_path: "/etc/vector/vector.yml"
    environment:
      LOGFLARE_API_KEY: "{{RANDOM_LOGFLARE_API_KEY}}"
      DOCKER_HOST: http://{{DOCKER_PROXY_HOST}}:2375/v1.45
    command:
      - /usr/local/bin/vector
      - --config
      - /etc/vector/vector.yml
    preferred_server_hostnames:
      - "{{SUPABASE_DEPLOYMENT_SERVER}}"
    docker_proxy_config:
      enabled: true
      permissions:
        ping: read
        version: read
        info: read
        events: read
        auth: none
        secrets: none
        build: none
        commit: none
        configs: none
        containers: read
        distribution: none
        exec: none
        grpc: none
        images: none
        networks: none
        nodes: none
        plugins: none
        services: none
        session: none
        swarm: none
        system: none
        tasks: none
        volumes: none
